{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words and Bag of NGrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll see  two features that we can add to our bag of words model, n-grams and stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. N-grams\n",
    "\n",
    "One new feature is the use of n-grams.  As we know, so far we have encoded our documents simply by looking at the *counts* of the words in a document.  However, it's probably not too much of a stretch to consider that groupings or phrases of words may also be worth encoding.  This is the thought behind n-grams, where we encode not only the occurrence of a single token, but also a sequence of tokens.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stop Words\n",
    "\n",
    "Stop words are just \"low value\" words, like \"the\", \"a\", that are eliminated from each document as they are believed to not assist  with helping a model perform classification.  \n",
    "\n",
    "In this lesson we'll learn both techniques with our newsgroups dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating Low Value Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading up our newsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "documents = pd.Series(newsgroups_train['data'])\n",
    "y = newsgroups_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at our first line of the document above:\n",
    "\n",
    "`WHAT car is this!?`\n",
    "\n",
    "We can see that some of the words, like `car`, are important for classifying this document.  But other words, like 'is', and 'this' occur fairly frequently, yet provide little information about the text.  One technique is simply to remove these words, called stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Stop words** are commonly occurring words believed to contain little information that we remove from our document vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing libraries often come with a list of predefined stop words.  For example, let's look at  some of the stop words that sklearn has predefined for us  in the `feature_extraction.text` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why',\n",
       " 'show',\n",
       " 'sometime',\n",
       " 'i',\n",
       " 'before',\n",
       " 'which',\n",
       " 'find',\n",
       " 'herein',\n",
       " 'together',\n",
       " 'became']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "stop_words = list(ENGLISH_STOP_WORDS)\n",
    "len(stop_words)\n",
    "# 318\n",
    "\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to ignore stop words in our CountVectorizer, we can ignore stop words with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizor = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "X = vectorizor.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x129796 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 263 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a sense of the how the CountVectorizer reduced the text to by calling `inverse_transform`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember there originally was the phrase, \"looked to be from the late 60s\", and now many of those words are gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['15', '60s', '70s', 'addition', 'body', 'bricklin', 'brought',\n",
       "        'bumper', 'called', 'car', 'college', 'day', 'door', 'doors',\n",
       "        'early', 'edu', 'engine', 'enlighten', 'funky', 'history', 'host',\n",
       "        'il', 'info', 'know', 'late', 'lerxst', 'lines', 'looked',\n",
       "        'looking', 'mail', 'maryland', 'model', 'neighborhood', 'nntp',\n",
       "        'organization', 'park', 'posting', 'production', 'rac3', 'really',\n",
       "        'rest', 'saw', 'separate', 'small', 'specs', 'sports', 'subject',\n",
       "        'tellme', 'thanks', 'thing', 'umd', 'university', 'wam',\n",
       "        'wondering', 'years'], dtype='<U180')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizor.inverse_transform(X[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature that we can incorporate in our bag of words model is the use of n-grams, called a bag of n-grams.  With n-grams we can not only categorize how often a specific word occurs, but how often a *sequence* occurs.  For example, in the code below we'll first break our text into sequences of length one and two, and then count how often each sequence occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizor = CountVectorizer(ngram_range = (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look towards the end of each document vector, we now see pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_vectors = vectorizor.fit_transform(documents[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['please mail', 'mail thanks', 'thanks il', 'il brought',\n",
       "       'brought to', 'to you', 'you by', 'by your', 'your neighborhood',\n",
       "       'neighborhood lerxst'], dtype='<U55')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizor.inverse_transform(n_gram_vectors[0])[0][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we feed this to a logistic regression model, it will now consider pairs of words in classifying the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And if we look at the beginning of the document vector we see the same list of individual words as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['from', 'lerxst', 'wam', 'umd', 'edu', 'where', 'my', 'thing',\n",
       "       'subject', 'what'], dtype='<U55')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizor.inverse_transform(n_gram_vectors[0])[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we used `CountVectorizer(ngram_range = (1, 2))` we'll now represent our document in terms of counts of individual words and sequences of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we train a model, our model will consider different sequences of words in making it's classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradeoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both stop words and n-grams, there are costs and benefits to employing them.  \n",
    "\n",
    "* Stop words\n",
    "\n",
    "With stop words, the idea of course, is to effectively reduce features that effectively are just noise, and thus reduce variance in our model.  But, how do we know that the features do not help a model discover an underlying signal?  We'll see that sometimes models perform better when we include the stop words.\n",
    "\n",
    "* Bag of N-grams\n",
    "\n",
    "While with N-grams, we can capture sequences of words, there is a downside in that the frequency of any specific sequence is quite rare.  In practice, this means that we generally will not employ an n-grams of length greater than 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about two different features that we can incorporate in our bag of words model.  The first is stop words, where we remove words that occur often, and do not add much information about a document.  And the second is n-grams where we encode a sequence of words, as well as individual words.  We include both features in our CountVectorizer with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizor = CountVectorizer(ngram_range = (1, 2), stop_words = ENGLISH_STOP_WORDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
