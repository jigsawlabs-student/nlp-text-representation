{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we saw a basic technique for turning our words into text.  We simply count up the number of times that each word occurs, let each word be represented by a separate dimension in a vector, and then perform a random forest.\n",
    "\n",
    "In this lesson, we'll see a different way to encode our words.  Doing so, as we'll see will give more weights to infrequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF stands for term frequency, inverse document frequency.  And we can think of the formula for TF-IDF literally as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tfidf = term\\_frequency * inverse\\_document\\_frequency $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words take the proportion of a word's frequency in a document, and multiply it by how rare that word is in general, and that should give us a metric of each word's importance in a document.  \n",
    "\n",
    "For example, if we have a document and both $armadillo$ and $house$ occur in twice in the document, we won't record these words as:\n",
    "\n",
    "`[2, 2]`, where the first element represents armadillo, but rather something like\n",
    "\n",
    "`[2.5, 2]` because armadillo is a less frequent word than house.  And therefore we want to add more weight to it.  Of course the formula is a little more complex than that, so let's move through it in the rest of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we previously defined the set of terms as follows.\n",
    "\n",
    "* Term - a term is an individual word\n",
    "* Document - a document is a set of words \n",
    "* Corpus - the set of all words in our dataset\n",
    "\n",
    "Now from this, we can start to piece together what term frequency-inverse document frequency means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a couple of our restaurant reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Wow I loved the pizza.`\n",
    "* `I loved the rolls, but the pizza crust was not good.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the reviews say 'loved'.  But, to represent the importance of 'loved' in the review, we can divide by the length of each document.  The 'loved' pizza consists of $\\frac{1}{5}$ of the first document, but only $\\frac{1}{11}$ of the second document.  This is called the term frequency.  \n",
    "\n",
    "$term\\_frequency = \\frac{term\\_count}{|D|} $\n",
    "\n",
    "> where $|D|$ is the length of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in general we don't think of our features as a count of words, but as a the proportion each word makes up our document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with inverse document frequency, we weight words based on how rare they are throughout the corpus.  The less frequent a word, the more we weight it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $IDF(term) = \\frac{\\text{total # docs}}{\\text{# of docs containing T}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if the word *pizza* appears in five out of one hundred documents in our corpus, we have an inverse document frequency of $IDF(pizza) = \\frac{100}{5} = 20$.\n",
    "\n",
    "If the word crust appears in one document in one hundred, inverse document frequency gives us $IDF(crust) = \\frac{100}{1} = 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Final Touches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can now express $TF-IDF$ as the following:\n",
    "\n",
    "$TF-IDF(w) = \\frac{ \\text{term count}}{\\text{document length}} * \\frac{\\text{total # of docs}}{\\text{docs with term}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And we can think of this as the word's *proportion* of the document, multiplied by the rarity of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last idea generally in TF-IDF.  The idea is that the rarity of a word changes exponentially.  So the word crust is likely to be much more rare than the word pizza throughout a document.  To prevent vastly different scores based on rarity, we generally take the update IDF to equal the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TF-IDF(w) = \\frac{ \\text{term count}}{\\text{document length}} * \\ln(\\frac{\\text{total # of docs}}{\\text{docs with term}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, applying the log, will tend to reduce our variance by impacting larger numbers more than smaller numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our newgroups dataset to explore some of the differences between tf-idf in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "documents = pd.Series(newsgroups_train['data'])\n",
    "y = newsgroups_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's look at the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now simply using the bag of words model gives us the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car           5\n",
       "lerxst        2\n",
       "edu           2\n",
       "umd           2\n",
       "wam           2\n",
       "bricklin      1\n",
       "info          1\n",
       "funky         1\n",
       "university    1\n",
       "separate      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "cv = CountVectorizer(stop_words = ENGLISH_STOP_WORDS)\n",
    "cv_vectors = cv.fit_transform(documents)\n",
    "pd.Series(cv_vectors[0].toarray()[0], cv.get_feature_names()).sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get a similar ranking with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(documents) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference is that `edu` is now ranked considerably lower in tf-idf.  This seems like a good change.  As a downside, we also notice that some typos are ranked higher.  Notice that both do a good job at pulling out the word `car` as an important feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car         0.381339\n",
       "lerxst      0.353835\n",
       "wam         0.259709\n",
       "umd         0.211868\n",
       "tellme      0.176918\n",
       "bricklin    0.167132\n",
       "rac3        0.160686\n",
       "funky       0.155872\n",
       "was         0.145347\n",
       "this        0.144473\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.Series(tfidf_vectorizer_vectors[0].toarray()[0], feature_names).sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[computing tf-idf in python](https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[naive bayes classifier](https://medium.com/@baemaek/text-mining-preprocess-and-naive-bayes-classifier-da0000f633b2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
