{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Text Normalization Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll become more familiar with the spacy library, and use it to perform text normalization, so that we can then use a vector space model like bag of words to represent our text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading our dataset of airline tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/jigsawlabs-student/nlp-text-representation/master/Tweets.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again assign the column `text` to the variable `documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at some of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica What @dhepburn said.',\n",
       " \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n",
       " '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
       " \"@VirginAmerica and it's a really big bad thing about it\",\n",
       " \"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\",\n",
       " '@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)',\n",
       " '@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP',\n",
       " \"@virginamerica Well, I didn't…but NOW I DO! :-D\",\n",
       " \"@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[document for document in documents][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spacy Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, from here, we simply initialized our count vectorizer.  But do not want to our CountVectorizer to represent similar words as completely different, we'll first use spacy to transform each word into it's corresponding lemma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by loading up the `en_core_web_sm` library of spacy, and assigning the model to the variable `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)\n",
    "# spacy.lang.en.English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's practice working with spacy on a single document.  Begin by selecting the first document, and then passing through this string into the model `nlp` (which we can think of as a parser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica What @dhepburn said.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc = documents[0]\n",
    "first_doc\n",
    "# '@VirginAmerica What @dhepburn said.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the parsed document to `first_spacy_doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_spacy_doc = nlp(first_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(first_spacy_doc)\n",
    "\n",
    "# spacy.tokens.doc.Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's display the lemma of each word.  Use list iteration to display the lemma of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica', 'what', '@dhepburn', 'say', '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in first_spacy_doc]\n",
    "\n",
    "# ['@VirginAmerica', 'what', '@dhepburn', 'say', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, notice that this included the punctuation mark, like a period.  Let's say that we don't want to include this, so we'll only include a token if spacy indicates that it is`is_alpha`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'say']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in first_spacy_doc if token.is_alpha]\n",
    "\n",
    "# ['what', 'say']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this got rid of too much information as it eliminated the twitter handles as well.  Instead, use the `is_punct` method to only exclude elements are punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica', 'what', '@dhepburn', 'say']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in first_spacy_doc if not token.is_punct]\n",
    "\n",
    "# ['@VirginAmerica', 'what', '@dhepburn', 'say']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is starting to look better.  Next, let's also get rid of stop words by using the `is_stop` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica', '@dhepburn', 'say']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in first_spacy_doc if not token.is_punct and not token.is_stop]\n",
    "# ['@VirginAmerica', '@dhepburn', 'say']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move onto incorporating our use of spacy with sklearn. First, we define a `tokenizer` function that takes in a document as a string, and returns a list of tokens.  We want our tokenizer to return a list of lemmas of the document.\n",
    "\n",
    "> **Do not** remove punctuation or stop words in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(document):\n",
    "    return [token.lemma_ for token in nlp(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica', 'what', '@dhepburn', 'say', '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(first_doc)\n",
    "\n",
    "# ['@VirginAmerica', '@dhepburn', 'say']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use our tokenizer to perform the task of tokenizing our document with our CountVectorizer.  Initialize our count vectorizer, setting the `tokenizer` as the function defined above.  Notice that if we wanted to remove stop words, we could have taken care of this in our tokenizer.\n",
    "\n",
    "Set the `ngram_range` to (1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(tokenizer = tokenizer, ngram_range= (1, 2))\n",
    "\n",
    "vectors = cv.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a sense of how well we performed this by looking at the returned vectors after an inverse transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['@virginamerica', 'what', '@dhepburn', 'say', '.',\n",
       "        '@virginamerica what', 'what @dhepburn', '@dhepburn say', 'say .'],\n",
       "       dtype='<U60'),\n",
       " array(['@virginamerica', '.', 'plus', '-PRON-', 'have', 'add',\n",
       "        'commercial', 'to', 'the', 'experience', '...', 'tacky',\n",
       "        '@virginamerica plus', 'plus -PRON-', '-PRON- have', 'have add',\n",
       "        'add commercial', 'commercial to', 'to the', 'the experience',\n",
       "        'experience ...', '... tacky', 'tacky .'], dtype='<U60'),\n",
       " array(['@virginamerica', 'to', '...', 'i', 'do', 'not', 'today', 'must',\n",
       "        'mean', 'need', 'take', 'another', 'trip', '!', '@virginamerica i',\n",
       "        'i do', 'do not', 'not today', 'today ...', '... must',\n",
       "        'must mean', 'mean i', 'i need', 'need to', 'to take',\n",
       "        'take another', 'another trip', 'trip !'], dtype='<U60')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vectors)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this improves the accuracy of our model at all.  Assign `airline_sentiment` to equal the variable `sentiment`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df.airline_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     neutral\n",
       "1    positive\n",
       "Name: airline_sentiment, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment[:2]\n",
    "\n",
    "# 0     neutral\n",
    "# 1    positive\n",
    "# Name: airline_sentiment, dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert the dataset to be numeric using the following mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the target to equal `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sentiment.map(sentiment_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our data into training and test data.  Set the `test_size` to .2, and stratify the data.  Set the `random_state` to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, y, stratify = y, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's train a logistic regression model setting the `random_state` to equal 2 and iterations to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter = 1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8106557377049181"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)\n",
    "# 0.8106557377049181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model performs similarly to it's performance before lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, we see that the features are slightly different, and we do not have recurring features like we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delay          1.452159\n",
       "suck           1.312443\n",
       "lose           1.186573\n",
       "not            1.068794\n",
       "hour           1.062909\n",
       "nothing        1.059326\n",
       "no             1.033789\n",
       "bad            1.021676\n",
       "terrible       0.976796\n",
       "the bad        0.906495\n",
       "on hold        0.885917\n",
       "bag            0.880149\n",
       "rude           0.855303\n",
       "why            0.834972\n",
       "hrs            0.805047\n",
       "fuck           0.790441\n",
       "since          0.783878\n",
       "cancel         0.783625\n",
       "stop           0.761884\n",
       "website        0.759926\n",
       ". help         0.747833\n",
       "just dm'd      0.739816\n",
       ":(             0.736653\n",
       "ridiculous     0.734676\n",
       "@united dme    0.727143\n",
       "give up        0.719622\n",
       "again          0.714931\n",
       "min            0.708890\n",
       "because        0.686110\n",
       "why do         0.683581\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lr.coef_[0], cv.get_feature_names()).sort_values(ascending = False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we worked with the spacy library and saw how we can use the `lemma_` `is_stop` and `is_punct` methods to select certain words, and return the lemma of our words.  We also saw how we can define our logic in a function and pass it through our vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Spacy Kaggle Tutorial](https://www.kaggle.com/honeysingh/spacy-tutorial)\n",
    "\n",
    "[Spellchecker](http://theautomatic.net/2019/12/10/3-packages-to-build-a-spell-checker-in-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
