{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll begin our journey with NLP.  Our first step is to understand some of the terms that will allow identify the components of an NLP problem.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by loading up some of our newsgroups dataset from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a little bit better if in addition to common words, we also promote rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(document_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values(document_tokens):\n",
    "    tokens = [token for doc in document_tokens for token in set(doc)]\n",
    "    counter_full = dictionary.doc2bow(tokens, allow_update=True)\n",
    "    idx_counts = dict(counter_full)\n",
    "    values = idx_counts.values()\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def doc_frequency(document_tokens):\n",
    "    count_values(document_tokens)\n",
    "    freq_pct = np.array(list(values))/len(document_tokens)\n",
    "    return np.around(freq_pct, decimals = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_doc_frequency(document_tokens):\n",
    "    values = count_values(document_tokens)\n",
    "    length = len(document_tokens)\n",
    "    inverse_freq = np.array(length/np.array(list(values)) + 1)\n",
    "    return np.around(inverse_freq, decimals = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = inverse_doc_frequency(document_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   54.6,    36.1,  2829.5, ..., 11315. , 11315. , 11315. ])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we would like to do is use the formula $idf*tf$, to reward words that are rare in the corpus but occur commonly in a particular document.  The problem right now, is that the idf score dominates the term frequency score.  Take a look at the numbers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {6: ['car', 4, 53.6, 214.4],\n",
    "#  19: ['it', 2, 35.1, 70.2],\n",
    "#  0: ['addition', 1, 2828.5, 2828.5],\n",
    "#  1: ['body', 1, 49.4, 49.4],\n",
    "#  2: ['bricklin', 1, 390.1, 390.1],\n",
    "#  3: ['brought', 1, 13.6, 13.6],\n",
    "#  4: ['bumper', 1, 19.9, 19.9],\n",
    "#  5: ['called', 1, 11.9, 11.9],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix by taking the log of the idf.  We'll update our formula accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_doc_frequency(document_tokens):\n",
    "    values = count_values(document_tokens)\n",
    "    length = len(document_tokens)\n",
    "    inverse_freq = np.log10(np.array(length/np.array(list(values)) + 1))\n",
    "    return np.around(inverse_freq, decimals = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll let's see how this affects our word scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(vector, document_tokens, top = 20):\n",
    "    idf = inverse_doc_frequency(document_tokens)\n",
    "    word_scores = [[idx, [dictionary.get(idx), num, inverse, num*inverse]] for (idx, num), inverse in zip(vector, idf)][:top]\n",
    "    word_scores.sort(key=lambda x: x[-1][-1], reverse=True)\n",
    "    return word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, ['car', 4, 4.0, 16.0]],\n",
       " [0, ['addition', 1, 7.9, 7.9]],\n",
       " [12, ['enlighten', 1, 7.3, 7.3]],\n",
       " [19, ['it', 2, 3.6, 7.2]],\n",
       " [2, ['bricklin', 1, 6.0, 6.0]],\n",
       " [11, ['engine', 1, 6.0, 6.0]],\n",
       " [8, ['door', 1, 5.0, 5.0]],\n",
       " [15, ['if', 1, 4.4, 4.4]],\n",
       " [7, ['day', 1, 4.3, 4.3]],\n",
       " [10, ['early', 1, 4.1, 4.1]]]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_first_doc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the original content of the first document, we can see that this does a fairly good job of selecting the most important components of a message post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with the second post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[57, ['edu', 2, 7.9, 15.8]],\n",
       " [78, ['poll', 2, 6.0, 12.0]],\n",
       " [44, ['add', 2, 4.0, 8.0]],\n",
       " [59, ['experiences', 2, 3.9, 7.8]],\n",
       " [48, ['brave', 1, 7.3, 7.3]],\n",
       " [52, ['clock', 2, 3.6, 7.2]],\n",
       " [94, ['washington', 2, 3.0, 6.0]],\n",
       " [47, ['base', 1, 6.0, 6.0]],\n",
       " [88, ['speed', 2, 2.7, 5.4]],\n",
       " [43, ['adapters', 1, 5.0, 5.0]]]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_tfidf(vectors[1], document_tokens)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\""
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Touches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Changing the term-frequency calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last component to TF-IDF, and that is to have the term frequency not be a count of the number of words in a post, but a percentage.  This can come in handy when say looking to match a post with a particular phrase.  For example, if we want to find posts that are most related to the word `speed`, then simply counting up the number of occurrences of speed would bias posts that are longer.  These posts are more likely to have more words in general - so we should discount each occurrence of a word the longer the post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by defining term frequency not as a simple count of the words, but as a percentage of each document.  So we redefine term frequency to be: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf = \\frac{num\\_terms}{|D|}$ \n",
    "\n",
    "where $|D|$ is the number of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(vector, document_tokens, top = 20):\n",
    "    idf = inverse_doc_frequency(document_tokens)\n",
    "    word_scores = [[idx, [dictionary.get(idx), num/len(vector), inverse, (num/len(vector))*inverse]] for (idx, num), inverse in zip(vector, idf)]\n",
    "    word_scores.sort(key=lambda x: x[-1][-1], reverse=True)\n",
    "    return word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, ['car', 0.09302325581395349, 4.0, 0.37209302325581395]],\n",
       " [22, ['lerxst', 0.023255813953488372, 8.6, 0.19999999999999998]],\n",
       " [37, ['tellme', 0.023255813953488372, 8.6, 0.19999999999999998]],\n",
       " [0, ['addition', 0.023255813953488372, 7.9, 0.18372093023255814]],\n",
       " [12, ['enlighten', 0.023255813953488372, 7.3, 0.1697674418604651]],\n",
       " [19, ['it', 0.046511627906976744, 3.6, 0.16744186046511628]],\n",
       " [2, ['bricklin', 0.023255813953488372, 6.0, 0.13953488372093023]],\n",
       " [11, ['engine', 0.023255813953488372, 6.0, 0.13953488372093023]],\n",
       " [29, ['neighborhood', 0.023255813953488372, 5.9, 0.1372093023255814]],\n",
       " [8, ['door', 0.023255813953488372, 5.0, 0.11627906976744186]]]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_tfidf(vectors[0], document_tokens)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Potentially remove words that only occur once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how to implement TF-IDF, largely from scratch.  Term Frequency Inverse Document Frequency simply multiplies those two components.  \n",
    "\n",
    "Term frequency is the number of times a given word occurs in a document, divided by the length of the document.  \n",
    "\n",
    "And inverse document frequency captures how rare a word is throughout the corpus.  We can think of this in two steps: \n",
    "\n",
    "1. calculate the frequency of a term throughout the entire corpus $\\frac{num t in C}{C}$ which gives a larger number the more frequent a word is\n",
    "\n",
    "2. Invert this number $\\frac{C}{num\\_t\\_in\\_C}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sklearn text data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "[ML Mastery Multiclass classification](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/#:~:text=Each%20binary%20classification%20model%20may,one%2Dversus%2Done%20classifier.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
