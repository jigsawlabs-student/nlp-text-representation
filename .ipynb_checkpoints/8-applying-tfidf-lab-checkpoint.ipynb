{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll move towards applying tf-idf to predict ratings of our Amazon reviews.  And we'll compare how well this performs versus our bag of words representation.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading up our coconut water reviews data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/jigsawlabs-student/nlp-text-representation/master/coconut_water.csv\"\n",
    "coconut_df = pd.read_csv(url, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first couple of reviews, we see that with each review we have a rating ranging from one to five, and corresponding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47836</th>\n",
       "      <td>47837</td>\n",
       "      <td>B004SRH2B6</td>\n",
       "      <td>AKACGHPVILE9R</td>\n",
       "      <td>Sophronia \"Euphemia\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1314144000</td>\n",
       "      <td>Switched to O.N.E.</td>\n",
       "      <td>Must admit the taste of O.N.E. coconut water i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47837</th>\n",
       "      <td>47838</td>\n",
       "      <td>B004SRH2B6</td>\n",
       "      <td>A2GO0AIHB846UX</td>\n",
       "      <td>vinny</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1313884800</td>\n",
       "      <td>WOW!!</td>\n",
       "      <td>I love this stuff!  Perfect blend of dark choc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId           ProfileName  \\\n",
       "47836  47837  B004SRH2B6   AKACGHPVILE9R  Sophronia \"Euphemia\"   \n",
       "47837  47838  B004SRH2B6  A2GO0AIHB846UX                 vinny   \n",
       "\n",
       "       HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "47836                     1                       1      1  1314144000   \n",
       "47837                     1                       1      5  1313884800   \n",
       "\n",
       "                  Summary                                               Text  \n",
       "47836  Switched to O.N.E.  Must admit the taste of O.N.E. coconut water i...  \n",
       "47837               WOW!!  I love this stuff!  Perfect blend of dark choc...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coconut_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assign our Text column to the variable documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = coconut_df.Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And assign the `Score` to the target $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = coconut_df.Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load up our spacy library, including the spacy stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next define a `spacy_tokenizer` that parses each document and returns a list of the lemmas of each words.  In addition, make sure that each word is lowercased, and strip it of any whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(document):\n",
    "    tokens = [word.lemma_.lower().strip() for word in nlp(document)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['must',\n",
       " 'admit',\n",
       " 'the',\n",
       " 'taste',\n",
       " 'of',\n",
       " 'o.n.e.',\n",
       " 'coconut',\n",
       " 'water',\n",
       " 'be',\n",
       " 'well',\n",
       " '.',\n",
       " '',\n",
       " 'take',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'to',\n",
       " 'get',\n",
       " 'through',\n",
       " 'the',\n",
       " 'supply',\n",
       " 'of',\n",
       " 'coconut',\n",
       " 'water',\n",
       " '.']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that this text has an empty string in there.  Change the tokenizer so that it only returns the text if the lowercased stripped lemma is not an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer_content(document):\n",
    "    tokens = [word.lemma_.lower().strip() for word in nlp(document) if word.lemma_.lower().strip()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['must',\n",
       " 'admit',\n",
       " 'the',\n",
       " 'taste',\n",
       " 'of',\n",
       " 'o.n.e.',\n",
       " 'coconut',\n",
       " 'water',\n",
       " 'be',\n",
       " 'well',\n",
       " '.',\n",
       " 'take',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'to',\n",
       " 'get',\n",
       " 'through',\n",
       " 'the',\n",
       " 'supply',\n",
       " 'of',\n",
       " 'coconut',\n",
       " 'water',\n",
       " '.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer_content(document)\n",
    "\n",
    "# ['must',\n",
    "#  'admit',\n",
    "#  'the',\n",
    "#  'taste',\n",
    "#  'of',\n",
    "#  'o.n.e.',\n",
    "#  'coconut',\n",
    "#  'water',\n",
    "#  'be',\n",
    "#  'well',\n",
    "#  '.',\n",
    "#  'take',\n",
    "#  'a',\n",
    "#  'long',\n",
    "#  'time',\n",
    "#  'to',\n",
    "#  'get',\n",
    "#  'through',\n",
    "#  'the',\n",
    "#  'supply',\n",
    "#  'of',\n",
    "#  'coconut',\n",
    "#  'water',\n",
    "#  '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we could also remove the periods by only returning tokens that return true for `is_alpha`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split the data into training and test sets, using the stratify parameter so that our data is evenly split.  Set the `test_size` to $.2$ and `random_state` to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because our documents are not yet numeric, use the variables `documents_train` and `documents_test` to assign the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "documents_train, documents_test, y_train, y_test = train_test_split(documents,  y, \n",
    "                                                                    stratify = y,\n",
    "                                                                    test_size = .2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's now transform the text using the TFIDF vectorizer.  Let's not use stop words, but do use an ngram range.  Specify an ngram range from (1, 2).  Use the `spacy_tokenizer` defined above as the tokenizer, and set `decode_error` to `ignore` so that any new patterns in the test set do not throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer, decode_error = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call `fit_transform` on the `documents_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_train = vectorizer.fit_transform(documents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 2741)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_train.shape\n",
    "# (364, 17326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that we have a training set of 364, each with over 17000 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember that TF-IDF gives us a score for each document based on how often the word appears and the rarity of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we can see how these words are encoded by passing through the trained data, along with the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>\"&lt;br</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>$1.00</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>...</th>\n",
       "      <th>zeco</th>\n",
       "      <th>zero</th>\n",
       "      <th>zica</th>\n",
       "      <th>zico</th>\n",
       "      <th>zico\"</th>\n",
       "      <th>zico-</th>\n",
       "      <th>zico.&lt;br</th>\n",
       "      <th>zicos</th>\n",
       "      <th>zito</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.188759</td>\n",
       "      <td>0.069990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.126623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.097159</td>\n",
       "      <td>0.270191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2741 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    !    \"  \"<br    #         $  $1.00         %         &  \\\n",
       "0  0.188759  0.069990  0.0   0.0  0.0  0.000000    0.0  0.000000  0.000000   \n",
       "1  0.126623  0.000000  0.0   0.0  0.0  0.000000    0.0  0.144329  0.000000   \n",
       "2  0.000000  0.000000  0.0   0.0  0.0  0.000000    0.0  0.000000  0.000000   \n",
       "3  0.097159  0.270191  0.0   0.0  0.0  0.000000    0.0  0.000000  0.000000   \n",
       "4  0.000000  0.078257  0.0   0.0  0.0  0.080189    0.0  0.000000  0.041222   \n",
       "\n",
       "     '  ...  zeco  zero  zica      zico  zico\"  zico-  zico.<br  zicos  zito  \\\n",
       "0  0.0  ...   0.0   0.0   0.0  0.062303    0.0    0.0       0.0    0.0   0.0   \n",
       "1  0.0  ...   0.0   0.0   0.0  0.125383    0.0    0.0       0.0    0.0   0.0   \n",
       "2  0.0  ...   0.0   0.0   0.0  0.050544    0.0    0.0       0.0    0.0   0.0   \n",
       "3  0.0  ...   0.0   0.0   0.0  0.096207    0.0    0.0       0.0    0.0   0.0   \n",
       "4  0.0  ...   0.0   0.0   0.0  0.087078    0.0    0.0       0.0    0.0   0.0   \n",
       "\n",
       "   zombie  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "\n",
       "[5 rows x 2741 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vectors = pd.DataFrame(vectors_train.toarray(), columns = vectorizer.get_feature_names())\n",
    "df_vectors[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see some of the words given the highest values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order          0.232879\n",
       "good!<br       0.225073\n",
       "/>cancelled    0.225073\n",
       "plastic        0.222743\n",
       "recipe         0.210369\n",
       "-pron-         0.193645\n",
       "today          0.191846\n",
       "auto           0.191846\n",
       "               0.188759\n",
       "delivery       0.174802\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vectors.iloc[0].sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "by          0.242019\n",
       "about       0.229052\n",
       "popular     0.211680\n",
       "talk        0.211680\n",
       "puree       0.211680\n",
       "mixer       0.211680\n",
       "40          0.201183\n",
       "everyone    0.175891\n",
       "as          0.172758\n",
       "light       0.171593\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vectors.iloc[1].sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(vectors_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we haven't yet encoded our test set, so use the `vectorizer` to apply the same transformation to the `documents_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_test = vectorizer.transform(documents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6304347826086957"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(vectors_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some of the most important features where the rating was 0.  Create a series that looks at the words with the highest parameters.  Select the top 20 largest features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plastic        1.244543\n",
       "concentrate    1.111643\n",
       "from           0.758279\n",
       "\"              0.738002\n",
       "buy            0.726199\n",
       ".              0.683141\n",
       "new            0.680499\n",
       "bottle         0.674728\n",
       "product        0.654184\n",
       "this           0.628896\n",
       "sip            0.616454\n",
       "ever           0.595003\n",
       "?              0.574656\n",
       "terrible       0.570207\n",
       "what           0.567574\n",
       "finish         0.544019\n",
       "disgusting     0.538367\n",
       "taste          0.509934\n",
       "horrible       0.471150\n",
       "money          0.462101\n",
       "dtype: float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model.coef_[0], vectorizer.get_feature_names()).sort_values(ascending = False)[:20]\n",
    "\n",
    "# plastic        1.244543\n",
    "# concentrate    1.111643\n",
    "# from           0.758279\n",
    "# \"              0.738002\n",
    "# buy            0.726199\n",
    "# .              0.683141\n",
    "# new            0.680499\n",
    "# bottle         0.674728\n",
    "# product        0.654184\n",
    "# this           0.628896\n",
    "# sip            0.616454\n",
    "# ever           0.595003\n",
    "# ?              0.574656\n",
    "# terrible       0.570207\n",
    "# what           0.567574\n",
    "# finish         0.544019\n",
    "# disgusting     0.538367\n",
    "# taste          0.509934\n",
    "# horrible       0.471150\n",
    "# money          0.462101\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select the ten features for the five star rated reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chocolate    1.574490\n",
       "!            1.399597\n",
       "great        1.395714\n",
       "love         1.034309\n",
       "and          0.740629\n",
       "delicious    0.727754\n",
       "favorite     0.596369\n",
       "more         0.554068\n",
       "healthy      0.533834\n",
       "for          0.533679\n",
       "dtype: float64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model.coef_[-1], vectorizer.get_feature_names()).sort_values(ascending = False)[:10]\n",
    "\n",
    "# chocolate    1.574490\n",
    "# !            1.399597\n",
    "# great        1.395714\n",
    "# love         1.034309\n",
    "# and          0.740629\n",
    "# delicious    0.727754\n",
    "# favorite     0.596369\n",
    "# more         0.554068\n",
    "# healthy      0.533834\n",
    "# for          0.533679\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we perform any better using the old technique of bag of words.  Use a CountVectorizer along with the `spacy_tokenizer` and set the `decode_error` to `ignore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_bow = CountVectorizer(tokenizer = spacy_tokenizer,\n",
    "                                    decode_error = 'ignore'\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform the `documents_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_bow_train = vectorizer_bow.fit_transform(documents_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And transform the `documents_test` by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_bow_test = vectorizer_bow.transform(documents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this performs.  Train a logistic regression model and score it on the test set.  Set the maximum number of iterations at `2000`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter = 2000)\n",
    "model.fit(vectors_bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6521739130434783"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(vectors_bow_test, y_test)\n",
    "# 0.6521739130434783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see a slight bump in the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can do any better using regularization.  Let's use the LogisticRegressionCV model for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a sense of how the regularization works from the documentation:\n",
    "> Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model_cv = LogisticRegressionCV(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(max_iter=2000)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.fit(vectors_bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6521739130434783"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.score(vectors_bow_test, y_test)\n",
    "# 0.6521739130434783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model used a range of regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000e+00, 0.00000e+00, 1.00000e-02, 5.00000e-02, 3.60000e-01,\n",
       "       2.78000e+00, 2.15400e+01, 1.66810e+02, 1.29155e+03, 1.00000e+04])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.Cs_.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that a relatively high one was chosen.  And this would shows that not a lot of regularization was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.7825594, 2.7825594, 2.7825594, 2.7825594, 2.7825594])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use bow with ngrams to see if this helps at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_bow_ngram = CountVectorizer(tokenizer = spacy_tokenizer,\n",
    "                                    decode_error = 'ignore',\n",
    "                                 ngram_range = (1, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_ngram_train = vectorizer_bow_ngram.fit_transform(documents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_ngram_test = vectorizer_bow_ngram.transform(documents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(max_iter=2000)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model_cv = LogisticRegressionCV(max_iter=2000)\n",
    "model_cv.fit(vectorizer_ngram_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6195652173913043"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.score(vectorizer_ngram_test, y_test)\n",
    "\n",
    "# 0.6195652173913043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We see that the score slighly decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at some of the most important features when some rates a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plastic             0.977640\n",
       "concentrate         0.891844\n",
       "new                 0.855902\n",
       "from concentrate    0.843802\n",
       "what                0.822078\n",
       "not buy             0.773720\n",
       "bottle              0.718263\n",
       "taste               0.672921\n",
       "this                0.671672\n",
       "ever                0.637346\n",
       "dtype: float64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_cv.coef_[0],vectorizer_bow_ngram.get_feature_names()).sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "great        1.430512\n",
       "chocolate    1.335300\n",
       "!            0.989884\n",
       "on           0.899702\n",
       "the good     0.816733\n",
       "with         0.806251\n",
       "all          0.789968\n",
       "and          0.749770\n",
       "delicious    0.749574\n",
       "'s           0.743754\n",
       "be -pron-    0.664415\n",
       "favorite     0.626084\n",
       "for          0.610332\n",
       "calorie      0.587700\n",
       "in           0.580491\n",
       "more         0.579082\n",
       "can          0.554747\n",
       "-pron- do    0.540177\n",
       "/>i          0.533376\n",
       "have be      0.513296\n",
       "dtype: float64"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_cv.coef_[-1],vectorizer_bow_ngram.get_feature_names()).sort_values(ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we used TF-IDF with our coconut water reviews dataset.  We saw that our model performed better by simply using a bow of words text representation.  We also saw little benefit from regularization.  We were able to get a sense of what people liked and did not like about the product by looking at the most significant features both when ngrams was used and when it wasn't."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
